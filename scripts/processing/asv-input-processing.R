# This script can be used to process file uploads from data providers
# to make them ready for import into the asv-postgrest db.
# See README.md for additional context.

# Assumes the following directory structure:
# ├── [datasetID - see tab/file event]
#   ├── input
#     ├── [filename].xlsx or [filename].tar.gz
#   ├── [this-script-name].R

# Edit dataset-specific sections 2 & 9!
# Then run script down to 'Add taxonomy' section, where you need to run ampliseq pipeline
# (https://nf-co.re/ampliseq) before finishing the script

################################################################################
# 1. Get required packages & start fresh
################################################################################

#install.packages("openxlsx")
library(openxlsx)
# Clean up environment
rm(list = ls())
# Set work dir to script location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
dir.create('output', showWarnings = FALSE)

################################################################################
# 2. Set and derive some dataset-specific variables - EDIT HERE, PLEASE!
################################################################################

# File uploaded by data provider
# uploaded_file <- 'input/jane.doe@univ.se_210902-212121_ampliseq.tar.gz'
uploaded_file <- 'input/jane.doe@univ.se_210902-212121_ampliseq.xlsx'

# Annotation file, to be generated by ampliseq, and added to input - see 'Add taxonomy section'
annotation_file <- 'input/annotation.tsv'

# Target prediction (comment out whole chunk if no prediction was performed)
# Criteria for predicting ASV as non-target (state even if prediction identified no non-targets)
target_criteria <- 'Assigned kingdom OR barrnap-positive'
# File listing ASVs predicted to be target gene (comment out if non-existing)
target_list <- 'input/rrna.bac.gff'

dataset_id <- 'SBDI-ASV-1'

# Bioatlas resource ID if this is an update of dataset already imported into Bioatlas:
bioatlas_resource_uid <- NA

# Derive output filenames
excel_out <- paste0('output/', dataset_id, "-adm.xlsx")
tar_out <- paste0('output/', dataset_id, "-adm.tar.gz")
fasta_out <- paste0('output/', dataset_id, '.fasta')

################################################################################
# 3. Define some functions
################################################################################

archiveToDFs <- function(archive_file){
  # Imports delimited text files from a compressed archive into data frames

  untar(uploaded_file, exdir='unpacked')
  for (xsv in list.files('unpacked', pattern="*.[ct]sv"))
  {
    name_parts <- strsplit(xsv, split="\\.")[[1]]
    if (name_parts[2] == 'tsv') sep = '\t' else sep = ','
    # Create data frames (envir needed for assign to work inside function)
    # but skip annotation that user may have submitted
    if (name_parts[1] != 'annotation')
      assign(name_parts[1],
             read.delim(paste0('unpacked/', xsv), sep = sep, dec=".", na.strings=""),
             envir = parent.frame())
  }
  # Delete intermediary dir
  unlink(paste0(getwd(),'/unpacked'), recursive = TRUE)
}

excelToDFs <- function(excel_file){
  # Imports Excel sheets into individual data frames

  # Get all sheet names in case user has renamed something
  my_sheets <- getSheetNames(excel_file)
  # Skip guide sheet
  my_sheets <- my_sheets[my_sheets != 'guide']
  for (sheet_name in my_sheets)
  {
    # Create data frames (envir needed for assign to work inside function)
    assign(sheet_name, read.xlsx(excel_file, sheet=sheet_name, detectDates = TRUE), envir = parent.frame())
  }
}

writeFasta <- function(data, filename){
  # Creates a fasta file from sequences and labels in a data frame
  fastaLines = c()
  for (rowNum in 1:nrow(data)){
    fastaLines = c(fastaLines, as.character(paste(">", data[rowNum,"asv_id_alias"], sep = "")))
    fastaLines = c(fastaLines,as.character(data[rowNum, "DNA_sequence"]))
  }
  fileConn<-file(filename)
  writeLines(fastaLines, fileConn)
  close(fileConn)
}

trimConvert <- function(df){
  # Removes any leading/trailing Unicode horizontal/vertical white space
  # in string cols, and converts to numeric, if possible
  df <- as.data.frame(lapply(df, function(col) {
    if (typeof(col) == "character") {
      col <- trimws(col, whitespace = "[\\h\\v]")
      col <- tryCatch({ as.numeric(col) }, warning = function(w) { col })
    }
    else { col }
  # Do not add 'X' to names starting with digit, e.g. '16S_1'
  }), check.names = FALSE)
  return(df)
}

################################################################################
# 4. Import uploaded data into dataframes, and clean up a bit
################################################################################

# Determine (apparent) input type
if(grepl('xlsx', uploaded_file, fixed = TRUE)){
  excelToDFs(uploaded_file)
} else {
  archiveToDFs(uploaded_file)
}

# Rename some dataframes
asv_table <- `asv-table`
rm(`asv-table`)
if (exists('emof-simple')){
  emof_simple <- `emof-simple`
  rm(`emof-simple`)
}

# Remove white space & format to numeric where possible
event <- trimConvert(event)
asv_table <- trimConvert(asv_table)
dna <- trimConvert(dna)
emof <- trimConvert(emof)

################################################################################
# 5. Add dataset metadata
################################################################################

dataset <- data.frame(datasetID = dataset_id, filename = basename(uploaded_file), bioatlas_resource_uid)
# And remove datasetID from event tab/file
event <- event[ , -which(names(event) == "datasetID")]

################################################################################
# 6. Handle emof data
################################################################################

# Only save EMOF rows that actually have data (and not just event IDs)
emof_data <- subset(emof, select = -c(eventID))
emof <- emof[rowSums(is.na(emof_data)) != ncol(emof_data),]
rm(emof_data)

# If emof-simple is used, transfer data to regular emof
if (nrow(emof) == 0 & exists('emof_simple')) {
  if (nrow(emof_simple) > 0){
    hdrs <- colnames(emof_simple)[colnames(emof_simple) != "eventID"]
    hdrs <- gsub(".", " ", hdrs, fixed = TRUE)
    # Make new emof row for each sample-variable combination
    for (id in emof_simple$eventID){
      for (i in 1:length(hdrs)) {
        mlist <- strsplit(hdrs[i], split='[()]')[[1]]
        type <- trimws(mlist[1])
        unit <- mlist[2]
        value <- emof_simple[emof_simple$eventID == id, i+1]
        emof[nrow(emof) + 1, ] <- c(id,type,NA,value,NA,unit,NA,NA,NA,NA,NA,NA)
      }
    }
  }
  rm(emof_simple)
}


################################################################################
# 7. Add taxonomy
################################################################################

# Make fasta for Ampliseq input
fasta_input <- asv_table[c("asv_id_alias", "DNA_sequence")]
writeFasta(fasta_input, fasta_out)

# [Run Ampliseq pipeline, and then add tsv output to input dir]

# Import Ampliseq output
# Disable comment char '#' to handle link/anchor in identification_references
annotation = read.delim(file = annotation_file, sep = '\t', header = TRUE,
                        dec = '.', comment.char = "", na.strings="")
# Assume target_gene is same for all ASVs here
annotation$annotation_target <- dna$target_gene[1]

################################################################################
# 8. Flag ASV:s based on target prediction outcome (Barrnap)
################################################################################

# If target prediction was made, flag ASVs as TRUE/FALSE based on this
if (exists('target_list')){
  if (file.exists(target_list)){
    true_targets = read.delim(file = target_list, sep = '\t', header = FALSE, skip = 1)
    annotation$target_criteria <- target_criteria
    annotation$target_prediction[(annotation$asv_id_alias %in% true_targets[,1])] <- TRUE
    annotation$target_prediction[!(annotation$asv_id_alias %in% true_targets[,1])] <- FALSE
  }
  # If no prediction was made, use defaults
} else {
  annotation$target_criteria <- 'None'
  annotation$target_prediction <- TRUE
}

################################################################################
# 9. Fix dataset-specifc problems, if any  - EDIT HERE, PLEASE!
################################################################################

# We concatenate datasetID with eventID in DwC output, so to avoid redundancy,
# remove datasetID + separator from eventID, if present
# sep <- ':'
# df_list <- list(event=event,dna=dna, emof=emof, emof_simple=emof_simple)
# df_list=lapply(list(event=event,dna=dna),
#               function(x) {x$eventID <- sub(paste0(dataset_id, sep), '', x$eventID);x})
# list2env(df_list,.GlobalEnv)

################################################################################
# 10. Create Excel file & compressed archive for import to asv-postgrest db

# We do both, as the Excel file is only accepted by pandas (python library)
# after you open and save it in Excel, for some reason.
# Tar import seems to work fine for direct import, though.
################################################################################

wb <- createWorkbook()
for (sheet in c('dataset', 'event', 'asv_table', 'dna', 'emof', 'annotation'))
{
  # Revert renaming in output (see above)
  if (sheet == 'asv_table') sheet_name <- 'asv-table'
  else if (sheet == 'dna') sheet_name <- 'mixs'
  else sheet_name <- sheet

  # Write df data to Excel workbook
  addWorksheet(wb, sheetName = sheet_name)
  writeData(wb, sheet_name, get(sheet), startRow = 1, startCol = 1)

  # and write same data to csv:s
  write.csv(get(sheet), paste0('output/', sheet_name, ".csv"), row.names=FALSE, na = "")
}
# Export [dataset_id]-adm.xlsx
saveWorkbook(wb, file = excel_out, overwrite = TRUE, )

# Also pack csv:s into [dataset_id]-adm.tar.gz
tfiles <- paste0('output/', list.files('output', pattern="*.csv"))
tar(tar_out, files = tfiles, compression = "gzip", tar='tar')
# Delete csv:s
unlink(tfiles)
